{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import numpy.random\n",
    "import numpy.linalg\n",
    "import scipy.io\n",
    "import scipy.stats\n",
    "import sklearn.metrics\n",
    "\n",
    "inNotebook = True # change this to True if you use a notebook\n",
    "def nextplot():\n",
    "    if inNotebook:\n",
    "        plt.figure()  # this creates a new plot\n",
    "    else:\n",
    "        plt.clf()     # and this clears the current one"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = scipy.io.loadmat('/tmp/spamData.mat')\n",
    "X = data['Xtrain']\n",
    "N = X.shape[0]\n",
    "D = X.shape[1]\n",
    "Xtest = data['Xtest']\n",
    "Ntest = Xtest.shape[0]\n",
    "y = data['ytrain'].squeeze().astype(int)\n",
    "ytest = data['ytest'].squeeze().astype(int)\n",
    "\n",
    "features = np.array([\n",
    "    \"word_freq_make\", \"word_freq_address\", \"word_freq_all\", \"word_freq_3d\",\n",
    "    \"word_freq_our\", \"word_freq_over\", \"word_freq_remove\", \"word_freq_internet\",\n",
    "    \"word_freq_order\", \"word_freq_mail\", \"word_freq_receive\", \"word_freq_will\",\n",
    "    \"word_freq_people\", \"word_freq_report\", \"word_freq_addresses\", \"word_freq_free\",\n",
    "    \"word_freq_business\", \"word_freq_email\", \"word_freq_you\", \"word_freq_credit\",\n",
    "    \"word_freq_your\", \"word_freq_font\", \"word_freq_000\", \"word_freq_money\",\n",
    "    \"word_freq_hp\", \"word_freq_hpl\", \"word_freq_george\", \"word_freq_650\",\n",
    "    \"word_freq_lab\", \"word_freq_labs\", \"word_freq_telnet\", \"word_freq_857\",\n",
    "    \"word_freq_data\", \"word_freq_415\", \"word_freq_85\", \"word_freq_technology\",\n",
    "    \"word_freq_1999\", \"word_freq_parts\", \"word_freq_pm\", \"word_freq_direct\",\n",
    "    \"word_freq_cs\", \"word_freq_meeting\", \"word_freq_original\", \"word_freq_project\",\n",
    "    \"word_freq_re\", \"word_freq_edu\", \"word_freq_table\", \"word_freq_conference\",\n",
    "    \"char_freq_;\", \"char_freq_(\", \"char_freq_[\", \"char_freq_!\",\n",
    "    \"char_freq_$\", \"char_freq_#\", \"capital_run_length_average\", \"capital_run_length_longest\",\n",
    "    \"capital_run_length_total\" ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Dataset Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DescribeResult(nobs=3065, minmax=(array([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n        0.,  0.,  1.,  1.,  1.]), array([  4.54000000e+00,   1.42800000e+01,   5.10000000e+00,\n         4.28100000e+01,   9.09000000e+00,   3.57000000e+00,\n         7.27000000e+00,   1.11100000e+01,   3.33000000e+00,\n         1.81800000e+01,   2.00000000e+00,   9.67000000e+00,\n         5.55000000e+00,   5.55000000e+00,   2.86000000e+00,\n         1.01600000e+01,   7.14000000e+00,   9.09000000e+00,\n         1.87500000e+01,   6.32000000e+00,   1.11100000e+01,\n         1.71000000e+01,   5.45000000e+00,   9.09000000e+00,\n         2.00000000e+01,   1.42800000e+01,   3.33300000e+01,\n         4.76000000e+00,   1.42800000e+01,   4.76000000e+00,\n         4.76000000e+00,   4.76000000e+00,   1.81800000e+01,\n         4.76000000e+00,   2.00000000e+01,   7.69000000e+00,\n         6.89000000e+00,   7.40000000e+00,   9.75000000e+00,\n         4.76000000e+00,   7.14000000e+00,   1.42800000e+01,\n         3.57000000e+00,   2.00000000e+01,   2.14200000e+01,\n         1.67000000e+01,   2.12000000e+00,   1.00000000e+01,\n         4.38500000e+00,   9.75200000e+00,   4.08100000e+00,\n         3.24780000e+01,   6.00300000e+00,   1.98290000e+01,\n         1.10250000e+03,   9.98900000e+03,   1.58410000e+04])), mean=array([  1.10818923e-01,   2.28486134e-01,   2.74153344e-01,\n         6.29690049e-02,   3.17787928e-01,   9.57553018e-02,\n         1.13546493e-01,   1.07216966e-01,   8.89233279e-02,\n         2.41719413e-01,   5.81305057e-02,   5.37432300e-01,\n         9.26231648e-02,   4.96639478e-02,   5.07210440e-02,\n         2.35334421e-01,   1.47197390e-01,   1.86600326e-01,\n         1.66121044e+00,   7.63066884e-02,   8.19592170e-01,\n         1.22727569e-01,   1.02006525e-01,   8.90799347e-02,\n         5.29800979e-01,   2.62071778e-01,   7.71507341e-01,\n         1.14323002e-01,   1.09487765e-01,   9.92952692e-02,\n         6.28156607e-02,   4.90342577e-02,   9.27471452e-02,\n         4.96019576e-02,   1.02156607e-01,   9.93050571e-02,\n         1.43285481e-01,   1.24274062e-02,   7.55921697e-02,\n         6.60456770e-02,   4.63360522e-02,   1.32176183e-01,\n         4.88580750e-02,   7.11876020e-02,   3.06590538e-01,\n         1.79794454e-01,   5.28874388e-03,   3.13768352e-02,\n         3.79543230e-02,   1.38396411e-01,   1.81830343e-02,\n         2.65470799e-01,   7.91275693e-02,   5.34218597e-02,\n         4.90062936e+00,   5.26750408e+01,   2.82203915e+02]), variance=array([  1.07094140e-01,   1.88742036e+00,   2.34317437e-01,\n         1.78161723e+00,   4.40325719e-01,   6.79193461e-02,\n         1.39844435e-01,   1.72001423e-01,   6.97247542e-02,\n         4.69800274e-01,   3.58302179e-02,   7.59167719e-01,\n         9.28365241e-02,   8.26118648e-02,   7.00470321e-02,\n         4.29393369e-01,   2.00636301e-01,   2.92991898e-01,\n         3.18992370e+00,   1.65626303e-01,   1.44315254e+00,\n         1.01505046e+00,   1.19749530e-01,   1.43862796e-01,\n         2.45800502e+00,   7.38036013e-01,   1.13920029e+01,\n         2.31010973e-01,   4.31507668e-01,   1.90528093e-01,\n         1.24671084e-01,   1.07425177e-01,   2.95159161e-01,\n         1.07745599e-01,   3.08154062e-01,   1.67896547e-01,\n         1.85791650e-01,   4.34829439e-02,   1.42525114e-01,\n         1.16865102e-01,   1.50361473e-01,   6.09903912e-01,\n         5.73945833e-02,   3.19259425e-01,   1.01935877e+00,\n         8.17471270e-01,   4.63438951e-03,   7.50333517e-02,\n         5.54612799e-02,   7.77968333e-02,   1.48045497e-02,\n         7.59181612e-01,   6.74541224e-02,   2.69600271e-01,\n         7.42311765e+02,   4.86573219e+04,   3.68952901e+05]), skewness=array([  5.92257918,   9.5555492 ,   2.94110789,  27.15035267,\n         4.22000271,   4.55490419,   6.21454549,  10.63604439,\n         4.44795353,   9.63368819,   5.1601559 ,   3.12797362,\n         7.99555783,  10.07103212,   6.44051978,   5.9017492 ,\n         5.71193665,   5.63845456,   1.6918398 ,   8.05102821,\n         2.36131511,   9.70708774,   5.74851972,  13.62929854,\n         5.51200726,   5.77490458,   5.72163481,   5.84582426,\n        11.30526457,   6.67894971,   8.78006633,  10.35563132,\n        16.1291286 ,  10.31146394,  17.98980105,   7.86085564,\n         5.29526945,  27.69555992,  10.51869112,   9.12514394,\n        12.60532735,   9.42688905,   7.88762618,  19.69945392,\n         9.63372543,   8.97501221,  18.94255005,  20.98217881,\n        14.12336521,  16.36382061,  21.32440567,  21.32959254,\n        10.88427173,  26.25786993,  27.34951229,  31.14016596,   9.80477376]), kurtosis=array([   51.71558405,    93.89016173,    13.18839908,   785.40163828,\n          28.69487647,    31.20576951,    66.53150801,   198.68010939,\n          28.29530115,   185.40607771,    34.48800593,    15.18712484,\n         109.66544541,   138.05561341,    44.19188958,    55.62892   ,\n          47.49151277,    52.75647121,     6.32523058,    77.87379384,\n           8.48736408,   103.7022867 ,    49.37553046,   272.09125904,\n          42.43992409,    49.41302953,    33.63974328,    39.86629858,\n         166.19735746,    53.12216402,    91.72439904,   124.79234055,\n         433.42661801,   123.97955409,   555.16708959,    86.72460731,\n          43.92486688,   865.39968623,   181.33012173,   100.87592785,\n         189.11563172,   111.21705016,    81.96093958,   567.75150773,\n         147.5283386 ,   107.79164424,   445.8361165 ,   634.57001982,\n         228.75884956,   499.07842266,   588.19774644,   688.05527222,\n         184.31757803,   851.48819158,   954.59095344,  1348.49464105,\n         183.78053905]))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# look some dataset statistics\n",
    "scipy.stats.describe(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "execution_count": 0,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/html": [
       "<div id='ea0a66e9-25b1-4969-84f0-419d6d7a3063'></div>"
      ],
      "text/plain": [
       "<div id='ea0a66e9-25b1-4969-84f0-419d6d7a3063'></div>"
      ]
     },
     "execution_count": 0,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f2e9600a898>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# plot the distribution of all features\n",
    "nextplot()\n",
    "densities = [ scipy.stats.gaussian_kde(X[:,j]) for j in range(D) ]\n",
    "xs = np.linspace(0,np.max(X),200)\n",
    "for j in range(D):\n",
    "    plt.plot(xs, densities[j](xs), label=j)\n",
    "plt.legend(ncol=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's compute z-scores; create two new variables Xz and Xtestz.\n",
    "\n",
    "# https://stackoverflow.com/a/30671796/6059889\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X)\n",
    "\n",
    "# Save mean and variance for later use\n",
    "mean = scaler.mean_\n",
    "var = scaler.var_\n",
    "\n",
    "Xz = scaler.transform(X)\n",
    "Xtestz = scaler.transform(Xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  1.85459768e-17   9.27298839e-18  -5.56379304e-17  -9.27298839e-18\n   5.56379304e-17   3.70919536e-17   0.00000000e+00  -7.41839072e-17\n   5.56379304e-17   0.00000000e+00  -1.85459768e-17  -2.43415945e-17\n  -4.63649420e-17   1.85459768e-17   1.85459768e-17   3.70919536e-17\n  -3.70919536e-17  -9.27298839e-17  -1.66913791e-16   9.27298839e-18\n   1.85459768e-17   9.27298839e-18  -5.56379304e-17  -1.85459768e-17\n  -6.49109188e-17  -3.70919536e-17  -1.85459768e-17   1.85459768e-17\n  -2.78189652e-17   4.63649420e-17  -1.85459768e-17   5.56379304e-17\n   0.00000000e+00  -1.85459768e-17   3.70919536e-17   1.85459768e-17\n  -9.27298839e-18   4.63649420e-18   1.85459768e-17   9.27298839e-18\n   2.31824710e-17  -2.78189652e-17  -9.27298839e-18   4.63649420e-18\n  -9.27298839e-18  -9.27298839e-18   1.39094826e-17  -2.78189652e-17\n  -3.70919536e-17  -6.49109188e-17   4.63649420e-18   3.70919536e-17\n  -3.70919536e-17   9.27298839e-18  -9.27298839e-18   9.27298839e-18\n  -7.41839072e-17]\n[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n  1.  1.  1.]\n[ -5.73600192e-02  -3.37389835e-02   4.02481250e-02   5.51233798e-03\n  -2.51229644e-02   1.67364997e-03   5.29785531e-03  -1.38875040e-02\n   1.29802458e-02  -1.00804532e-02   2.68026912e-02   1.46804853e-02\n   1.28455840e-02   9.34193448e-02  -1.71666713e-02   6.17841473e-02\n  -3.08405298e-02  -1.02710095e-02   1.49139906e-03   6.82438979e-02\n  -2.45179646e-02  -4.53675036e-03  -3.12737328e-03   4.09841941e-02\n   3.76515934e-02   1.15494599e-02  -3.73018154e-03   6.55839018e-02\n  -4.82178216e-02   2.44089391e-02   1.64408852e-02  -1.81514851e-02\n   2.47142980e-02  -1.61248615e-02   1.75684573e-02  -1.33686432e-02\n  -4.40153254e-02   1.11212504e-02   2.40959269e-02  -1.06211719e-02\n  -2.06246544e-02   6.23149655e-04  -3.45073187e-02   4.24615929e-02\n  -1.59254291e-02   9.77429328e-05   6.85319587e-03   5.38462415e-03\n   7.89156240e-03   6.81007462e-03  -2.97234292e-02   1.23785037e-02\n  -3.82610483e-02  -5.29891640e-02   3.19860888e-02  -6.82149671e-03\n   5.35333143e-03]\n[ 0.61068019  0.64746339  1.25293677  1.2774661   1.08119249  1.31173762\n  1.28697678  0.80611698  1.33973062  0.65533893  1.40034314  0.93450565\n  0.92877323  2.0728468   0.86981179  2.75968123  0.94816223  0.88879741\n  0.96502082  2.70171906  0.99741759  1.1098788   1.07414603  2.08336518\n  1.40816544  1.19772845  0.9862879   1.76326753  0.44704368  1.28342341\n  1.91457064  1.01476883  1.14073258  1.02208023  0.75850361  0.89687605\n  0.89454052  1.35876298  1.97554069  1.14319113  0.60370645  0.89279613\n  0.61835224  1.633395    1.01236044  1.04674566  1.76525404  1.2642542\n  1.20646248  0.81912474  0.42556335  0.62984245  0.68863812  0.05099329\n  2.06687781  0.34306778  0.98979083]\n1925261.156\n"
     ]
    }
   ],
   "source": [
    "# Let's check. Xz and Xtestz refer to the normalized datasets just created. We\n",
    "# will use them throughout.\n",
    "print(np.mean(Xz, axis=0))                       # should be all 0\n",
    "print(np.var(Xz, axis=0))                       # should be all 1\n",
    "print(np.mean(Xtestz, axis=0))                 # what do you get here?\n",
    "print(np.var(Xtestz, axis=0))\n",
    "print(np.sum(Xz**3))                            # should be: 1925261.15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "execution_count": 0,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/html": [
       "<div id='0d4bd1df-12bf-453c-9f4c-1fb0524ef544'></div>"
      ],
      "text/plain": [
       "<div id='0d4bd1df-12bf-453c-9f4c-1fb0524ef544'></div>"
      ]
     },
     "execution_count": 0,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f2e93d4e7f0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Explore the normalized data\n",
    "\n",
    "nextplot()\n",
    "densities = [ scipy.stats.gaussian_kde(Xz[:,j]) for j in range(D) ]\n",
    "xs = np.linspace(- np.median(var) * 6,np.median(var) * 6,200)\n",
    "for j in range(D):\n",
    "    plt.plot(xs, densities[j](xs), label=j)\n",
    "plt.legend(ncol=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Maximum Likelihood Estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logsumexp(x):\n",
    "    \"\"\"Computes log(sum(exp(x)).\n",
    "\n",
    "    Uses offset trick to reduce risk of numeric over- or underflow. When x is a\n",
    "    1D ndarray, computes logsumexp of its entries. When x is a 2D ndarray,\n",
    "    computes logsumexp of each row.\n",
    "\n",
    "    Keyword arguments:\n",
    "    x : a 1D or 2D ndarray\n",
    "    \"\"\"\n",
    "    offset = np.max(x, axis=0)\n",
    "    return offset + np.log(np.sum(np.exp(x-offset), axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the logistic function. Make sure it operates on both scalars\n",
    "# and vectors.\n",
    "\n",
    "# https://stackoverflow.com/a/25164452/6059889\n",
    "\n",
    "def sigma(x):\n",
    "    return 1 / (1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.5, array([ 0.26894142,  0.5       ,  0.73105858])]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this should give:\n",
    "# [0.5, array([0.26894142, 0.5, 0.73105858])]\n",
    "[ sigma(0), sigma(np.array([-1,0,1])) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the logarithm of the logistic function. Make sure it operates on both\n",
    "# scalars and vectors. Perhaps helpful: isinstance(x, np.ndarray).\n",
    "def logsigma (x):\n",
    "    if isinstance(x, np.ndarray):\n",
    "        return np.vectorize(np.log)(sigma(x))\n",
    "    else:\n",
    "        return np.log(sigma(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-0.69314718055994529, array([-1.31326169, -0.69314718, -0.31326169])]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this should give:\n",
    "# [-0.69314718055994529, array([-1.31326169, -0.69314718, -0.31326169])]\n",
    "[ logsigma(0), logsigma(np.array([-1,0,1])) ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2b Log-likelihood and gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def l(y, X, w):\n",
    "    \"\"\"Log-likelihood of the logistic regression model.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    y : ndarray of shape (N,)\n",
    "        Binary labels (either 0 or 1).\n",
    "    X : ndarray of shape (N,D)\n",
    "        Design matrix.\n",
    "    w : ndarray of shape (D,)\n",
    "        Weight vector.\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this should give:\n",
    "# -47066.641667825766\n",
    "l(y, Xz, np.linspace(-5,5,D))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dl(y,X,w):\n",
    "    \"\"\"Gradient of the log-likelihood of the logistic regression model.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    y : ndarray of shape (N,)\n",
    "        Binary labels (either 0 or 1).\n",
    "    X : ndarray of shape (N,D)\n",
    "        Design matrix.\n",
    "    w : ndarray of shape (D,)\n",
    "        Weight vector.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    ndarray of shape (D,)\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this should give:\n",
    "# array([  551.33985842,   143.84116318,   841.83373606,   156.87237578,\n",
    "#          802.61217579,   795.96202907,   920.69045803,   621.96516752,\n",
    "#          659.18724769,   470.81259805,   771.32406968,   352.40325626,\n",
    "#          455.66972482,   234.36600888,   562.45454038,   864.83981264,\n",
    "#          787.19723703,   649.48042176,   902.6478154 ,   544.00539886,\n",
    "#         1174.78638035,   120.3598967 ,   839.61141672,   633.30453444,\n",
    "#         -706.66815087,  -630.2039816 ,  -569.3451386 ,  -527.50996698,\n",
    "#         -359.53701083,  -476.64334832,  -411.60620464,  -375.11950586,\n",
    "#         -345.37195689,  -376.22044258,  -407.31761977,  -456.23251936,\n",
    "#         -596.86960184,  -107.97072355,  -394.82170044,  -229.18125598,\n",
    "#         -288.46356547,  -362.13402385,  -450.87896465,  -277.03932676,\n",
    "#         -414.99293368,  -452.28771693,  -167.54649092,  -270.9043748 ,\n",
    "#         -252.20140951,  -357.72497343,  -259.12468742,   418.35938483,\n",
    "#          604.54173228,    43.10390907,   152.24258478,   378.16731033,\n",
    "#          416.12032881])\n",
    "dl(y, Xz, np.linspace(-5,5,D))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2c Gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you don't need to modify this function\n",
    "def optimize(obj_up, theta0, nepochs=50, eps0=0.01):\n",
    "    \"\"\"Iteratively minimize a function.\n",
    "\n",
    "    We use it here to run either gradient descent or stochastic gradient\n",
    "    descent, using arbitrarly optimization criteria.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    obj_up  : a tuple of form (f, update) containing two functions f and update.\n",
    "              f(theta) computes the value of the objective function.\n",
    "              update(theta,eps) performs a parameter update with step size eps\n",
    "              and returns the result.\n",
    "    theta0  : ndarray of shape (D,)\n",
    "              Initial parameter vector.\n",
    "    nepochs : int\n",
    "              How many epochs (calls to update) to run.\n",
    "    eps0    : float\n",
    "              Initial step size.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    A triple consisting of the fitted parameter vector, the values of the\n",
    "    objective function after every epoch, and the step sizes that were used.\n",
    "    \"\"\"\n",
    "\n",
    "    f, update = obj_up\n",
    "\n",
    "    # initialize results\n",
    "    theta = theta0\n",
    "    values = np.zeros(nepochs+1)\n",
    "    eps = np.zeros(nepochs+1)\n",
    "    values[0] = f(theta0)\n",
    "    eps[0] = eps0\n",
    "\n",
    "    # now run the update function nepochs times\n",
    "    for epoch in range(nepochs):\n",
    "        print(\"Epoch {:3d}: f={:10.3f}, eps={:10.9f}\".format(epoch, values[epoch], eps[epoch]))\n",
    "        theta = update(theta, eps[epoch])\n",
    "\n",
    "        # we use the bold driver heuristic\n",
    "        values[epoch+1] = f(theta)\n",
    "        if (values[epoch] < values[epoch+1]):\n",
    "            eps[epoch+1] = eps[epoch]/2.\n",
    "        else:\n",
    "            eps[epoch+1] = eps[epoch]*1.05\n",
    "\n",
    "    # all done\n",
    "    print(\"Result after {} epochs: f={}\".format(nepochs, values[-1]))\n",
    "    return theta, values, eps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the objective and update function for one gradient-descent epoch for\n",
    "# fitting an MLE estimate of logistic regression with gradient descent (should\n",
    "# return a tuple of two functions; see optimize)\n",
    "def gd(y,X):\n",
    "    # YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this should give\n",
    "# [47066.641667825766,\n",
    "#  array([  4.13777838e+01,  -1.56745627e+01,   5.75882538e+01,\n",
    "#           1.14225143e+01,   5.54249703e+01,   5.99229049e+01,\n",
    "#           7.11220141e+01,   4.84761728e+01,   5.78067289e+01,\n",
    "#           4.54794720e+01,   7.14638492e+01,   1.51369386e+01,\n",
    "#           3.36375739e+01,   2.15061217e+01,   5.78014255e+01,\n",
    "#           6.72743066e+01,   7.00829312e+01,   5.29328088e+01,\n",
    "#           6.16042473e+01,   5.50018510e+01,   8.94624817e+01,\n",
    "#           2.74784480e+01,   8.51763599e+01,   5.60363965e+01,\n",
    "#          -2.55865589e+01,  -1.53788213e+01,  -4.67015412e+01,\n",
    "#          -2.50356570e+00,  -3.85357592e+00,  -2.21819155e+00,\n",
    "#           3.32098671e+00,   3.86933390e+00,  -2.00309898e+01,\n",
    "#           3.84684492e+00,  -2.19847927e-01,  -1.29775457e+00,\n",
    "#          -1.28374302e+01,  -2.78303173e+00,  -5.61671182e+00,\n",
    "#           1.73657121e+01,  -6.81197570e+00,  -1.20249002e+01,\n",
    "#           2.65789491e+00,  -1.39557852e+01,  -2.01135653e+01,\n",
    "#          -2.72134051e+01,  -9.45952961e-01,  -1.02239111e+01,\n",
    "#           1.52794293e-04,  -5.18938123e-01,  -3.19717561e+00,\n",
    "#           4.62953437e+01,   7.87893022e+01,   1.88618651e+01,\n",
    "#           2.85195027e+01,   5.04698358e+01,   6.41240689e+01])\n",
    "f, update = gd(y, Xz)\n",
    "[ f(np.linspace(-5,5,D)),\n",
    "  update(np.linspace(-5,-5,D), .1) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can run gradient descent!\n",
    "numpy.random.seed(0)\n",
    "w0 = np.random.normal(size=D)\n",
    "wz_gd, vz_gd, ez_gd = optimize(gd(y,Xz), w0, nepochs=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at how gradient descent made progess\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2d Stochastic gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sgdepoch(y,X,w,eps):\n",
    "    \"\"\"Run one SGD epoch and return the updated weight vector. \"\"\"\n",
    "    # Run N stochastic gradient steps (without replacement). Do not rescale each\n",
    "    # step by factor N (i.e., proceed differntly than in the lecture slides).\n",
    "    # YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# when you run this multiple times, with 50% probability you should get the\n",
    "# following result (there is one other result which is very close):\n",
    "# array([ -3.43689655e+02,  -1.71161311e+02,  -5.71093536e+02,\n",
    "#         -5.16478220e+01,   4.66294348e+02,  -3.71589878e+02,\n",
    "#          5.21493183e+02,   1.25699230e+03,   8.33804130e+02,\n",
    "#          5.63185399e+02,   1.32761302e+03,  -2.64104011e+02,\n",
    "#          7.10693307e+02,  -1.75497331e+02,  -1.94174427e+02,\n",
    "#          1.11641507e+02,  -3.30817509e+02,  -3.46754913e+02,\n",
    "#          8.48722111e+02,  -1.89136304e+02,  -4.25693844e+02,\n",
    "#         -1.23084189e+02,  -2.95894797e+02,  -2.35789333e+02,\n",
    "#         -3.38695243e+02,  -3.05642830e+02,  -2.28975383e+02,\n",
    "#         -2.38075137e+02,  -1.66702530e+02,  -2.27341599e+02,\n",
    "#         -1.77575620e+02,  -1.49093855e+02,  -1.70028859e+02,\n",
    "#         -1.50243833e+02,  -1.82986008e+02,  -2.41143708e+02,\n",
    "#         -3.31047159e+02,  -5.79991185e+01,  -1.98477863e+02,\n",
    "#         -1.91264948e+02,  -1.17371919e+02,  -1.66953779e+02,\n",
    "#         -2.01472565e+02,  -1.23330949e+02,  -3.00857740e+02,\n",
    "#         -1.95853348e+02,  -7.44868073e+01,  -1.11172370e+02,\n",
    "#         -1.57618226e+02,  -1.25729512e+00,  -1.45536466e+02,\n",
    "#         -1.43362438e+02,  -3.00429708e+02,  -9.84391082e+01,\n",
    "#         -4.54152047e+01,  -5.26492232e+01,  -1.45175427e+02])\n",
    "sgdepoch(y[1:3],Xz[1:3,:],np.linspace(-5,5,D),1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the objective and update function for one gradient-descent epoch for\n",
    "# fitting an MLE estimate of logistic regression with stochastic gradient descent\n",
    "# (should return a tuple of two functions; see optimize)\n",
    "def sgd(y,X):\n",
    "    # YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with 50% probability, you should get:\n",
    "# [40.864973045695081,\n",
    "#  array([ -3.43689655e+02,  -1.71161311e+02,  -5.71093536e+02,\n",
    "#          -5.16478220e+01,   4.66294348e+02,  -3.71589878e+02,\n",
    "#           5.21493183e+02,   1.25699230e+03,   8.33804130e+02,\n",
    "#           5.63185399e+02,   1.32761302e+03,  -2.64104011e+02,\n",
    "#           7.10693307e+02,  -1.75497331e+02,  -1.94174427e+02,\n",
    "#           1.11641507e+02,  -3.30817509e+02,  -3.46754913e+02,\n",
    "#           8.48722111e+02,  -1.89136304e+02,  -4.25693844e+02,\n",
    "#          -1.23084189e+02,  -2.95894797e+02,  -2.35789333e+02,\n",
    "#          -3.38695243e+02,  -3.05642830e+02,  -2.28975383e+02,\n",
    "#          -2.38075137e+02,  -1.66702530e+02,  -2.27341599e+02,\n",
    "#          -1.77575620e+02,  -1.49093855e+02,  -1.70028859e+02,\n",
    "#          -1.50243833e+02,  -1.82986008e+02,  -2.41143708e+02,\n",
    "#          -3.31047159e+02,  -5.79991185e+01,  -1.98477863e+02,\n",
    "#          -1.91264948e+02,  -1.17371919e+02,  -1.66953779e+02,\n",
    "#          -2.01472565e+02,  -1.23330949e+02,  -3.00857740e+02,\n",
    "#          -1.95853348e+02,  -7.44868073e+01,  -1.11172370e+02,\n",
    "#          -1.57618226e+02,  -1.25729512e+00,  -1.45536466e+02,\n",
    "#          -1.43362438e+02,  -3.00429708e+02,  -9.84391082e+01,\n",
    "#          -4.54152047e+01,  -5.26492232e+01,  -1.45175427e+02])]\n",
    "f, update = sgd(y[1:3], Xz[1:3,:])\n",
    "[ f(np.linspace(-5,5,D)),\n",
    "  update(np.linspace(-5,5,D), 1000) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can run stochastic gradient descent!\n",
    "wz_sgd, vz_sgd, ez_sgd = optimize(sgd(y,Xz), w0, nepochs=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2e Compare GD and SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(Xtest, w):\n",
    "    \"\"\"Returns vector of predicted confidence values for logistic regression with\n",
    "weight vector w.\"\"\"\n",
    "    # YOUR CODE HERE\n",
    "\n",
    "def classify(Xtest, w):\n",
    "    \"\"\"Returns 0/1 vector of predicted class labels for logistic regression with\n",
    "weight vector w.\"\"\"\n",
    "    # YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: confusion matrix\n",
    "yhat = predict(Xtestz, wz_gd)\n",
    "ypred = classify(Xtestz, wz_gd)\n",
    "print( sklearn.metrics.classification_report(ytest, ypred) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: precision-recall curve (with annotated thresholds)\n",
    "nextplot()\n",
    "precision, recall, thresholds = sklearn.metrics.precision_recall_curve(ytest, yhat)\n",
    "plt.plot(recall, precision)\n",
    "for x in np.linspace(0,1,10,endpoint=False):\n",
    "    index = int(x * (precision.size-1))\n",
    "    plt.text(recall[index], precision[index], \"{:3.2f}\".format(thresholds[index]))\n",
    "plt.xlabel(\"Recall\")\n",
    "plt.ylabel(\"Precision\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 Maximum Aposteriori Estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4a Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def l_l2(y,X,w,lambda_):\n",
    "    \"\"\"Log-density of posterior of logistic regression with weights w and L2\n",
    "regularization parameter lambda_\"\"\"\n",
    "    # YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this should give:\n",
    "# [-47066.641667825766, -47312.623810682911]\n",
    "[ l_l2(y, Xz, np.linspace(-5,5,D),0), l_l2(y, Xz, np.linspace(-5,5,D),1) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dl_l2(y,X,w,lambda_):\n",
    "    \"\"\"Gradient of log-density of posterior of logistic regression with weights w\n",
    "and L2 regularization parameter lambda_.\"\"\"\n",
    "    # YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this should give:\n",
    "# [array([  551.33985842,   143.84116318,   841.83373606,   156.87237578,\n",
    "#           802.61217579,   795.96202907,   920.69045803,   621.96516752,\n",
    "#           659.18724769,   470.81259805,   771.32406968,   352.40325626,\n",
    "#           455.66972482,   234.36600888,   562.45454038,   864.83981264,\n",
    "#           787.19723703,   649.48042176,   902.6478154 ,   544.00539886,\n",
    "#          1174.78638035,   120.3598967 ,   839.61141672,   633.30453444,\n",
    "#          -706.66815087,  -630.2039816 ,  -569.3451386 ,  -527.50996698,\n",
    "#          -359.53701083,  -476.64334832,  -411.60620464,  -375.11950586,\n",
    "#          -345.37195689,  -376.22044258,  -407.31761977,  -456.23251936,\n",
    "#          -596.86960184,  -107.97072355,  -394.82170044,  -229.18125598,\n",
    "#          -288.46356547,  -362.13402385,  -450.87896465,  -277.03932676,\n",
    "#          -414.99293368,  -452.28771693,  -167.54649092,  -270.9043748 ,\n",
    "#          -252.20140951,  -357.72497343,  -259.12468742,   418.35938483,\n",
    "#           604.54173228,    43.10390907,   152.24258478,   378.16731033,\n",
    "#           416.12032881]),\n",
    "#  array([  556.33985842,   148.66259175,   846.4765932 ,   161.33666149,\n",
    "#           806.89789007,   800.06917193,   924.61902946,   625.71516752,\n",
    "#           662.75867626,   474.20545519,   774.5383554 ,   355.43897054,\n",
    "#           458.52686767,   237.04458031,   564.95454038,   867.16124121,\n",
    "#           789.34009417,   651.44470748,   904.43352968,   545.61254171,\n",
    "#          1176.21495178,   121.6098967 ,   840.68284529,   634.19739158,\n",
    "#          -705.95386516,  -629.66826731,  -568.98799574,  -527.33139555,\n",
    "#          -359.53701083,  -476.82191975,  -411.9633475 ,  -375.65522015,\n",
    "#          -346.08624261,  -377.11329972,  -408.38904835,  -457.48251936,\n",
    "#          -598.29817327,  -109.57786641,  -396.60741472,  -231.14554169,\n",
    "#          -290.60642261,  -364.45545242,  -453.37896465,  -279.71789819,\n",
    "#          -417.85007654,  -455.32343122,  -170.76077664,  -274.29723194,\n",
    "#          -255.77283808,  -361.47497343,  -263.05325885,   414.25224198,\n",
    "#           600.25601799,    38.63962335,   147.59972763,   373.34588176,\n",
    "#           411.12032881])]\n",
    "[ dl_l2(y, Xz, np.linspace(-5,5,D),0), dl_l2(y, Xz, np.linspace(-5,5,D),1) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now define the (f,update) tuple for optimize for logistic regression, L2\n",
    "# regularization, and gradient descent\n",
    "def gd_l2(y,X,lambda_):\n",
    "    # YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's run!\n",
    "lambda_ = 100\n",
    "wz_gd_l2, vz_gd_l2, ez_gd_l2 = optimize(gd_l2(y,Xz,lambda_), w0, nepochs=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4b Effect of Prior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4c Composition of Weight Vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 Exploration (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all yours"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
