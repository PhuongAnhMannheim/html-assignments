{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import numpy.random\n",
    "import numpy.linalg\n",
    "import scipy.io\n",
    "import scipy.stats\n",
    "import sklearn.metrics\n",
    "\n",
    "inNotebook = True # change this to True if you use a notebook\n",
    "def nextplot():\n",
    "    if inNotebook:\n",
    "        plt.figure()  # this creates a new plot\n",
    "    else:\n",
    "        plt.clf()     # and this clears the current one"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = scipy.io.loadmat('/tmp/spamData.mat')\n",
    "X = data['Xtrain']\n",
    "N = X.shape[0]\n",
    "D = X.shape[1]\n",
    "Xtest = data['Xtest']\n",
    "Ntest = Xtest.shape[0]\n",
    "y = data['ytrain'].squeeze().astype(int)\n",
    "ytest = data['ytest'].squeeze().astype(int)\n",
    "\n",
    "features = np.array([\n",
    "    \"word_freq_make\", \"word_freq_address\", \"word_freq_all\", \"word_freq_3d\",\n",
    "    \"word_freq_our\", \"word_freq_over\", \"word_freq_remove\", \"word_freq_internet\",\n",
    "    \"word_freq_order\", \"word_freq_mail\", \"word_freq_receive\", \"word_freq_will\",\n",
    "    \"word_freq_people\", \"word_freq_report\", \"word_freq_addresses\", \"word_freq_free\",\n",
    "    \"word_freq_business\", \"word_freq_email\", \"word_freq_you\", \"word_freq_credit\",\n",
    "    \"word_freq_your\", \"word_freq_font\", \"word_freq_000\", \"word_freq_money\",\n",
    "    \"word_freq_hp\", \"word_freq_hpl\", \"word_freq_george\", \"word_freq_650\",\n",
    "    \"word_freq_lab\", \"word_freq_labs\", \"word_freq_telnet\", \"word_freq_857\",\n",
    "    \"word_freq_data\", \"word_freq_415\", \"word_freq_85\", \"word_freq_technology\",\n",
    "    \"word_freq_1999\", \"word_freq_parts\", \"word_freq_pm\", \"word_freq_direct\",\n",
    "    \"word_freq_cs\", \"word_freq_meeting\", \"word_freq_original\", \"word_freq_project\",\n",
    "    \"word_freq_re\", \"word_freq_edu\", \"word_freq_table\", \"word_freq_conference\",\n",
    "    \"char_freq_;\", \"char_freq_(\", \"char_freq_[\", \"char_freq_!\",\n",
    "    \"char_freq_$\", \"char_freq_#\", \"capital_run_length_average\", \"capital_run_length_longest\",\n",
    "    \"capital_run_length_total\" ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Dataset Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look some dataset statistics\n",
    "scipy.stats.describe(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the distribution of all features\n",
    "nextplot()\n",
    "densities = [ scipy.stats.gaussian_kde(X[:,j]) for j in range(D) ]\n",
    "xs = np.linspace(0,np.max(X),200)\n",
    "for j in range(D):\n",
    "    plt.plot(xs, densities[j](xs), label=j)\n",
    "plt.legend(ncol=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this plots is not really helpful; go now explore further\n",
    "# YOUR CODE HERE\n",
    "\n",
    "# Try different bandwidths and scales\n",
    "\n",
    "# [0, 0.3]\n",
    "nextplot()\n",
    "densities = [ scipy.stats.gaussian_kde(X[:,j]) for j in range(D) ]\n",
    "xs = np.linspace(0, 0.3,200)\n",
    "for j in range(D):\n",
    "    plt.plot(xs, densities[j](xs), label=j)\n",
    "plt.legend(ncol=5)\n",
    "\n",
    "# Silverman\n",
    "nextplot()\n",
    "densities = [ scipy.stats.gaussian_kde(X[:,j], bw_method='silverman') for j in range(D) ]\n",
    "xs = np.linspace(0, 0.3,200)\n",
    "for j in range(D):\n",
    "    plt.plot(xs, densities[j](xs), label=j)\n",
    "plt.legend(ncol=5)\n",
    "\n",
    "# bw_method=0.01\n",
    "nextplot()\n",
    "densities = [ scipy.stats.gaussian_kde(X[:,j], bw_method=0.01) for j in range(D) ]\n",
    "xs = np.linspace(0, 0.3,200)\n",
    "for j in range(D):\n",
    "    plt.plot(xs, densities[j](xs), label=j)\n",
    "plt.legend(ncol=5)\n",
    "\n",
    "# bw_method=0.5\n",
    "nextplot()\n",
    "densities = [ scipy.stats.gaussian_kde(X[:,j]) for j in range(D) ]\n",
    "xs = np.linspace(0, 0.3,200)\n",
    "for j in range(D):\n",
    "    plt.plot(xs, densities[j](xs), label=j)\n",
    "plt.legend(ncol=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's compute z-scores; create two new variables Xz and Xtestz.\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X)\n",
    "\n",
    "mean = scaler.mean_\n",
    "var = scaler.var_\n",
    "\n",
    "Xz = scaler.transform(X)\n",
    "Xtestz = scaler.transform(Xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's check. Xz and Xtestz refer to the normalized datasets just created. We\n",
    "# will use them throughout.\n",
    "np.mean(Xz, axis=0)                       # should be all 0\n",
    "np.var(Xz, axis=0)                        # should be all 1\n",
    "np.mean(Xtestz, axis=0)                   # what do you get here?\n",
    "np.var(Xtestz, axis=0)\n",
    "\n",
    "np.sum(Xz**3)                             # should be: 1925261.15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore the normalized data\n",
    "# YOUR CODE HERE\n",
    "\n",
    "# Show some statistics\n",
    "scipy.stats.describe(Xz)\n",
    "\n",
    "# Plot the kde\n",
    "nextplot()\n",
    "densities = [ scipy.stats.gaussian_kde(Xz[:,j]) for j in range(D) ]\n",
    "xs = np.linspace(- np.median(var) * 6,np.median(var) * 6,200)\n",
    "for j in range(D):\n",
    "    plt.plot(xs, densities[j](xs), label=j)\n",
    "plt.legend(ncol=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Maximum Likelihood Estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logsumexp(x):\n",
    "    \"\"\"Computes log(sum(exp(x)).\n",
    "\n",
    "    Uses offset trick to reduce risk of numeric over- or underflow. When x is a\n",
    "    1D ndarray, computes logsumexp of its entries. When x is a 2D ndarray,\n",
    "    computes logsumexp of each row.\n",
    "\n",
    "    Keyword arguments:\n",
    "    x : a 1D or 2D ndarray\n",
    "    \"\"\"\n",
    "    offset = np.max(x, axis=0)\n",
    "    return offset + np.log(np.sum(np.exp(x-offset), axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the logistic function. Make sure it operates on both scalars\n",
    "# and vectors.\n",
    "\n",
    "def sigma(x):\n",
    "    return 1 / (1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this should give:\n",
    "# [0.5, array([0.26894142, 0.5, 0.73105858])]\n",
    "[ sigma(0), sigma(np.array([-1,0,1])) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the logarithm of the logistic function. Make sure it operates on both\n",
    "# scalars and vectors. Perhaps helpful: isinstance(x, np.ndarray).\n",
    "def logsigma (x):\n",
    "    if isinstance(x, np.ndarray):\n",
    "        return np.vectorize(np.log)(sigma(x))\n",
    "    else:\n",
    "        return np.log(sigma(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this should give:\n",
    "# [-0.69314718055994529, array([-1.31326169, -0.69314718, -0.31326169])]\n",
    "[ logsigma(0), logsigma(np.array([-1,0,1])) ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2b Log-likelihood and gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replacer(x):\n",
    "    if x == 1.0:\n",
    "        return 0.999\n",
    "    else:\n",
    "        return x\n",
    "\n",
    "def l(y, X, w):\n",
    "    # https://datascience.stackexchange.com/questions/9302/the-cross-entropy-error-function-in-neural-networks\n",
    "    \n",
    "    \"\"\"Log-likelihood of the logistic regression model.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    y : ndarray of shape (N,)\n",
    "        Binary labels (either 0 or 1).\n",
    "    X : ndarray of shape (N,D)\n",
    "        Design matrix.\n",
    "    w : ndarray of shape (D,)\n",
    "        Weight vector.\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    p = sigma(X @ w)\n",
    "    y_1 = y @ np.log(p)\n",
    "    # This may not be exactly the correct answer, but avoids numerical instability in sigma(X @ w)\n",
    "    y_0 = (1 - y) @ (1 - np.log(1 - np.vectorize(replacer)(p)))      \n",
    "    return y_1 + y_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this should give:\n",
    "# -47066.641667825766\n",
    "l(y, Xz, np.linspace(-5,5,D))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dl(y,X,w):\n",
    "    \"\"\"Gradient of the log-likelihood of the logistic regression model.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    y : ndarray of shape (N,)\n",
    "        Binary labels (either 0 or 1).\n",
    "    X : ndarray of shape (N,D)\n",
    "        Design matrix.\n",
    "    w : ndarray of shape (D,)\n",
    "        Weight vector.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    ndarray of shape (D,)\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    return -(sigma(X @ w) - y) @ X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this should give:\n",
    "# array([  551.33985842,   143.84116318,   841.83373606,   156.87237578,\n",
    "#          802.61217579,   795.96202907,   920.69045803,   621.96516752,\n",
    "#          659.18724769,   470.81259805,   771.32406968,   352.40325626,\n",
    "#          455.66972482,   234.36600888,   562.45454038,   864.83981264,\n",
    "#          787.19723703,   649.48042176,   902.6478154 ,   544.00539886,\n",
    "#         1174.78638035,   120.3598967 ,   839.61141672,   633.30453444,\n",
    "#         -706.66815087,  -630.2039816 ,  -569.3451386 ,  -527.50996698,\n",
    "#         -359.53701083,  -476.64334832,  -411.60620464,  -375.11950586,\n",
    "#         -345.37195689,  -376.22044258,  -407.31761977,  -456.23251936,\n",
    "#         -596.86960184,  -107.97072355,  -394.82170044,  -229.18125598,\n",
    "#         -288.46356547,  -362.13402385,  -450.87896465,  -277.03932676,\n",
    "#         -414.99293368,  -452.28771693,  -167.54649092,  -270.9043748 ,\n",
    "#         -252.20140951,  -357.72497343,  -259.12468742,   418.35938483,\n",
    "#          604.54173228,    43.10390907,   152.24258478,   378.16731033,\n",
    "#          416.12032881])\n",
    "dl(y, Xz, np.linspace(-5,5,D))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2c Gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you don't need to modify this function\n",
    "def optimize(obj_up, theta0, nepochs=50, eps0=0.01):\n",
    "    \"\"\"Iteratively minimize a function.\n",
    "\n",
    "    We use it here to run either gradient descent or stochastic gradient\n",
    "    descent, using arbitrarly optimization criteria.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    obj_up  : a tuple of form (f, update) containing two functions f and update.\n",
    "              f(theta) computes the value of the objective function.\n",
    "              update(theta,eps) performs a parameter update with step size eps\n",
    "              and returns the result.\n",
    "    theta0  : ndarray of shape (D,)\n",
    "              Initial parameter vector.\n",
    "    nepochs : int\n",
    "              How many epochs (calls to update) to run.\n",
    "    eps0    : float\n",
    "              Initial step size.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    A triple consisting of the fitted parameter vector, the values of the\n",
    "    objective function after every epoch, and the step sizes that were used.\n",
    "    \"\"\"\n",
    "\n",
    "    f, update = obj_up\n",
    "\n",
    "    # initialize results\n",
    "    theta = theta0\n",
    "    values = np.zeros(nepochs+1)\n",
    "    eps = np.zeros(nepochs+1)\n",
    "    values[0] = f(theta0)\n",
    "    eps[0] = eps0\n",
    "\n",
    "    # now run the update function nepochs times\n",
    "    for epoch in range(nepochs):\n",
    "        print(\"Epoch {:3d}: f={:10.3f}, eps={:10.9f}\".format(epoch, values[epoch], eps[epoch]))\n",
    "        theta = update(theta, eps[epoch])\n",
    "\n",
    "        # we use the bold driver heuristic\n",
    "        values[epoch+1] = f(theta)\n",
    "        if (values[epoch] < values[epoch+1]):\n",
    "            eps[epoch+1] = eps[epoch]/2.\n",
    "        else:\n",
    "            eps[epoch+1] = eps[epoch]*1.05\n",
    "\n",
    "    # all done\n",
    "    print(\"Result after {} epochs: f={}\".format(nepochs, values[-1]))\n",
    "    return theta, values, eps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the objective and update function for one gradient-descent epoch for\n",
    "# fitting an MLE estimate of logistic regression with gradient descent (should\n",
    "# return a tuple of two functions; see optimize)\n",
    "def gd(y,X):\n",
    "    # YOUR CODE HERE\n",
    "    def f(w):\n",
    "        return -l(y, X, w)\n",
    "    def update(w, a):\n",
    "        return w + a * dl(y, X, w)\n",
    "    return [f, update]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this should give\n",
    "# [47066.641667825766,\n",
    "#  array([  4.13777838e+01,  -1.56745627e+01,   5.75882538e+01,\n",
    "#           1.14225143e+01,   5.54249703e+01,   5.99229049e+01,\n",
    "#           7.11220141e+01,   4.84761728e+01,   5.78067289e+01,\n",
    "#           4.54794720e+01,   7.14638492e+01,   1.51369386e+01,\n",
    "#           3.36375739e+01,   2.15061217e+01,   5.78014255e+01,\n",
    "#           6.72743066e+01,   7.00829312e+01,   5.29328088e+01,\n",
    "#           6.16042473e+01,   5.50018510e+01,   8.94624817e+01,\n",
    "#           2.74784480e+01,   8.51763599e+01,   5.60363965e+01,\n",
    "#          -2.55865589e+01,  -1.53788213e+01,  -4.67015412e+01,\n",
    "#          -2.50356570e+00,  -3.85357592e+00,  -2.21819155e+00,\n",
    "#           3.32098671e+00,   3.86933390e+00,  -2.00309898e+01,\n",
    "#           3.84684492e+00,  -2.19847927e-01,  -1.29775457e+00,\n",
    "#          -1.28374302e+01,  -2.78303173e+00,  -5.61671182e+00,\n",
    "#           1.73657121e+01,  -6.81197570e+00,  -1.20249002e+01,\n",
    "#           2.65789491e+00,  -1.39557852e+01,  -2.01135653e+01,\n",
    "#          -2.72134051e+01,  -9.45952961e-01,  -1.02239111e+01,\n",
    "#           1.52794293e-04,  -5.18938123e-01,  -3.19717561e+00,\n",
    "#           4.62953437e+01,   7.87893022e+01,   1.88618651e+01,\n",
    "#           2.85195027e+01,   5.04698358e+01,   6.41240689e+01])\n",
    "f, update = gd(y, Xz)\n",
    "[ f(np.linspace(-5,5,D)),\n",
    "  update(np.linspace(-5,-5,D), .1) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can run gradient descent!\n",
    "numpy.random.seed(0)\n",
    "w0 = np.random.normal(size=D)\n",
    "wz_gd, vz_gd, ez_gd = optimize(gd(y,Xz), w0, nepochs=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at how gradient descent made progess\n",
    "# YOUR CODE HERE\n",
    "nextplot()\n",
    "xs = np.linspace(0,100,vz_gd.size)\n",
    "for j in range(D):\n",
    "    plt.plot(xs, vz_gd, label=j, ls=':')\n",
    "    \n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Error')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2d Stochastic gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sgdepoch(y,X,w,eps):\n",
    "    \"\"\"Run one SGD epoch and return the updated weight vector. \"\"\"\n",
    "    # Run N stochastic gradient steps (without replacement). Do not rescale each\n",
    "    # step by factor N (i.e., proceed differently than in the lecture slides).\n",
    "    # YOUR CODE HERE\n",
    "    xs = np.array_split(X, N)\n",
    "    ys = np.array_split(y, N)\n",
    "    d = w    \n",
    "    for i in range(N):\n",
    "        d = d + 0.5 * eps * dl(ys[i], xs[i], w)\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# when you run this multiple times, with 50% probability you should get the\n",
    "# following result (there is one other result which is very close):\n",
    "# array([ -3.43689655e+02,  -1.71161311e+02,  -5.71093536e+02,\n",
    "#         -5.16478220e+01,   4.66294348e+02,  -3.71589878e+02,\n",
    "#          5.21493183e+02,   1.25699230e+03,   8.33804130e+02,\n",
    "#          5.63185399e+02,   1.32761302e+03,  -2.64104011e+02,\n",
    "#          7.10693307e+02,  -1.75497331e+02,  -1.94174427e+02,\n",
    "#          1.11641507e+02,  -3.30817509e+02,  -3.46754913e+02,\n",
    "#          8.48722111e+02,  -1.89136304e+02,  -4.25693844e+02,\n",
    "#         -1.23084189e+02,  -2.95894797e+02,  -2.35789333e+02,\n",
    "#         -3.38695243e+02,  -3.05642830e+02,  -2.28975383e+02,\n",
    "#         -2.38075137e+02,  -1.66702530e+02,  -2.27341599e+02,\n",
    "#         -1.77575620e+02,  -1.49093855e+02,  -1.70028859e+02,\n",
    "#         -1.50243833e+02,  -1.82986008e+02,  -2.41143708e+02,\n",
    "#         -3.31047159e+02,  -5.79991185e+01,  -1.98477863e+02,\n",
    "#         -1.91264948e+02,  -1.17371919e+02,  -1.66953779e+02,\n",
    "#         -2.01472565e+02,  -1.23330949e+02,  -3.00857740e+02,\n",
    "#         -1.95853348e+02,  -7.44868073e+01,  -1.11172370e+02,\n",
    "#         -1.57618226e+02,  -1.25729512e+00,  -1.45536466e+02,\n",
    "#         -1.43362438e+02,  -3.00429708e+02,  -9.84391082e+01,\n",
    "#         -4.54152047e+01,  -5.26492232e+01,  -1.45175427e+02])\n",
    "sgdepoch(y[1:3],Xz[1:3,:],np.linspace(-5,5,D),1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the objective and update function for one gradient-descent epoch for\n",
    "# fitting an MLE estimate of logistic regression with stochastic gradient descent\n",
    "# (should return a tuple of two functions; see optimize)\n",
    "def sgd(y,X):\n",
    "    # YOUR CODE HERE\n",
    "    def f(w):\n",
    "        return -l(y, X, w)\n",
    "    def update(w, eps):\n",
    "        return sgdepoch(y, X, w, eps)\n",
    "    return [f, update]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with 50% probability, you should get:\n",
    "# [40.864973045695081,\n",
    "#  array([ -3.43689655e+02,  -1.71161311e+02,  -5.71093536e+02,\n",
    "#          -5.16478220e+01,   4.66294348e+02,  -3.71589878e+02,\n",
    "#           5.21493183e+02,   1.25699230e+03,   8.33804130e+02,\n",
    "#           5.63185399e+02,   1.32761302e+03,  -2.64104011e+02,\n",
    "#           7.10693307e+02,  -1.75497331e+02,  -1.94174427e+02,\n",
    "#           1.11641507e+02,  -3.30817509e+02,  -3.46754913e+02,\n",
    "#           8.48722111e+02,  -1.89136304e+02,  -4.25693844e+02,\n",
    "#          -1.23084189e+02,  -2.95894797e+02,  -2.35789333e+02,\n",
    "#          -3.38695243e+02,  -3.05642830e+02,  -2.28975383e+02,\n",
    "#          -2.38075137e+02,  -1.66702530e+02,  -2.27341599e+02,\n",
    "#          -1.77575620e+02,  -1.49093855e+02,  -1.70028859e+02,\n",
    "#          -1.50243833e+02,  -1.82986008e+02,  -2.41143708e+02,\n",
    "#          -3.31047159e+02,  -5.79991185e+01,  -1.98477863e+02,\n",
    "#          -1.91264948e+02,  -1.17371919e+02,  -1.66953779e+02,\n",
    "#          -2.01472565e+02,  -1.23330949e+02,  -3.00857740e+02,\n",
    "#          -1.95853348e+02,  -7.44868073e+01,  -1.11172370e+02,\n",
    "#          -1.57618226e+02,  -1.25729512e+00,  -1.45536466e+02,\n",
    "#          -1.43362438e+02,  -3.00429708e+02,  -9.84391082e+01,\n",
    "#          -4.54152047e+01,  -5.26492232e+01,  -1.45175427e+02])]\n",
    "f, update = sgd(y[1:3], Xz[1:3,:])\n",
    "[ f(np.linspace(-5,5,D)),\n",
    "  update(np.linspace(-5,5,D), 1000) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can run stochastic gradient descent!\n",
    "wz_sgd, vz_sgd, ez_sgd = optimize(sgd(y,Xz), w0, nepochs=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2e Compare GD and SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "\n",
    "# look at how both made progress\n",
    "nextplot()\n",
    "xs = np.linspace(0,100,vz_gd.size)\n",
    "plt.plot(xs, vz_gd, label='gd', ls=':')\n",
    "    \n",
    "xs = np.linspace(0,100,vz_sgd.size)\n",
    "plt.plot(xs, vz_sgd, label='sgd', ls='--')\n",
    "    \n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Error')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(Xtest, w):\n",
    "    \"\"\"Returns vector of predicted confidence values for logistic regression with\n",
    "weight vector w.\"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    return sigma(Xtest @ w)\n",
    "    \n",
    "def classify(Xtest, w):\n",
    "    \"\"\"Returns 0/1 vector of predicted class labels for logistic regression with\n",
    "weight vector w.\"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    def predictor(x):\n",
    "        return 1 if x > 0.5 else 0\n",
    "    return np.vectorize(predictor)(predict(Xtest, w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\nGradient descent statistics')\n",
    "print(scipy.stats.describe(wz_gd))\n",
    "\n",
    "print('\\nStochastic gradient descent statistics')\n",
    "print(scipy.stats.describe(wz_sgd))\n",
    "\n",
    "# Commented out for better readability of the notebook\n",
    "# print('\\nParameters of gradient descent')\n",
    "# for j in range(wz_gd.size):\n",
    "#    print(str(j) + '  ' + str(wz_gd[j]) + '  ' + features[j])\n",
    "\n",
    "# print('\\nParameters of stochastic gradient descent')\n",
    "# for j in range(wz_sgd.size):\n",
    "#     print(str(j) + '  ' + str(wz_sgd[j]) + '  ' + features[j])\n",
    "\n",
    "# Example: confusion matrix\n",
    "yhat = predict(Xtestz, wz_gd)\n",
    "ypred = classify(Xtestz, wz_gd)\n",
    "print('\\nGradient descent prediction accuracy\\n')\n",
    "print( sklearn.metrics.classification_report(ytest, ypred) )\n",
    "\n",
    "# Print sgd confusion matrix\n",
    "syhat = predict(Xtestz, wz_sgd)\n",
    "sypred = classify(Xtestz, wz_sgd)\n",
    "print('\\nStochastic gradient descent prediction accuracy\\n')\n",
    "print( sklearn.metrics.classification_report(ytest, sypred) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: precision-recall curve (with annotated thresholds)\n",
    "nextplot()\n",
    "precision, recall, thresholds = sklearn.metrics.precision_recall_curve(ytest, yhat)\n",
    "plt.plot(recall, precision)\n",
    "for x in np.linspace(0,1,10,endpoint=False):\n",
    "    index = int(x * (precision.size-1))\n",
    "    plt.text(recall[index], precision[index], \"{:3.2f}\".format(thresholds[index]))\n",
    "plt.xlabel(\"Recall\")\n",
    "plt.ylabel(\"Precision\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 Maximum Aposteriori Estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4a Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def l_l2(y,X,w,lambda_):\n",
    "    \"\"\"Log-density of posterior of logistic regression with weights w and L2\n",
    "regularization parameter lambda_\"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    return l(y, X, w) + lambda_ * np.sum(w @ w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this should give:\n",
    "# [-47066.641667825766, -47312.623810682911]\n",
    "# Follow up mistake from a wrong log-likelihood function. Logic should be correct\n",
    "[ l_l2(y, Xz, np.linspace(-5,5,D),0), l_l2(y, Xz, np.linspace(-5,5,D),1) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dl_l2(y,X,w,lambda_):\n",
    "    \"\"\"Gradient of log-density of posterior of logistic regression with weights w\n",
    "and L2 regularization parameter lambda_.\"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    return dl(y, X, w) - lambda_ * w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this should give:\n",
    "# [array([  551.33985842,   143.84116318,   841.83373606,   156.87237578,\n",
    "#           802.61217579,   795.96202907,   920.69045803,   621.96516752,\n",
    "#           659.18724769,   470.81259805,   771.32406968,   352.40325626,\n",
    "#           455.66972482,   234.36600888,   562.45454038,   864.83981264,\n",
    "#           787.19723703,   649.48042176,   902.6478154 ,   544.00539886,\n",
    "#          1174.78638035,   120.3598967 ,   839.61141672,   633.30453444,\n",
    "#          -706.66815087,  -630.2039816 ,  -569.3451386 ,  -527.50996698,\n",
    "#          -359.53701083,  -476.64334832,  -411.60620464,  -375.11950586,\n",
    "#          -345.37195689,  -376.22044258,  -407.31761977,  -456.23251936,\n",
    "#          -596.86960184,  -107.97072355,  -394.82170044,  -229.18125598,\n",
    "#          -288.46356547,  -362.13402385,  -450.87896465,  -277.03932676,\n",
    "#          -414.99293368,  -452.28771693,  -167.54649092,  -270.9043748 ,\n",
    "#          -252.20140951,  -357.72497343,  -259.12468742,   418.35938483,\n",
    "#           604.54173228,    43.10390907,   152.24258478,   378.16731033,\n",
    "#           416.12032881]),\n",
    "#  array([  556.33985842,   148.66259175,   846.4765932 ,   161.33666149,\n",
    "#           806.89789007,   800.06917193,   924.61902946,   625.71516752,\n",
    "#           662.75867626,   474.20545519,   774.5383554 ,   355.43897054,\n",
    "#           458.52686767,   237.04458031,   564.95454038,   867.16124121,\n",
    "#           789.34009417,   651.44470748,   904.43352968,   545.61254171,\n",
    "#          1176.21495178,   121.6098967 ,   840.68284529,   634.19739158,\n",
    "#          -705.95386516,  -629.66826731,  -568.98799574,  -527.33139555,\n",
    "#          -359.53701083,  -476.82191975,  -411.9633475 ,  -375.65522015,\n",
    "#          -346.08624261,  -377.11329972,  -408.38904835,  -457.48251936,\n",
    "#          -598.29817327,  -109.57786641,  -396.60741472,  -231.14554169,\n",
    "#          -290.60642261,  -364.45545242,  -453.37896465,  -279.71789819,\n",
    "#          -417.85007654,  -455.32343122,  -170.76077664,  -274.29723194,\n",
    "#          -255.77283808,  -361.47497343,  -263.05325885,   414.25224198,\n",
    "#           600.25601799,    38.63962335,   147.59972763,   373.34588176,\n",
    "#           411.12032881])]\n",
    "[ dl_l2(y, Xz, np.linspace(-5,5,D),0), dl_l2(y, Xz, np.linspace(-5,5,D),1) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now define the (f,update) tuple for optimize for logistic regression, L2\n",
    "# regularization, and gradient descent\n",
    "def gd_l2(y,X,lambda_):\n",
    "    # YOUR CODE HERE\n",
    "    def f(w):\n",
    "        return -l_l2(y, X, w, lambda_)\n",
    "    def update(w, eps):\n",
    "        return w + eps * dl_l2(y, X, w, lambda_)\n",
    "    return [f, update]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's run!\n",
    "lambda_ = 100\n",
    "wz_gd_100, vz_gd_100, ez_gd_100 = optimize(gd_l2(y,Xz,lambda_), w0, nepochs=500)\n",
    "\n",
    "lambda_ = 50\n",
    "wz_gd_50, vz_gd_50, ez_gd_50 = optimize(gd_l2(y,Xz,lambda_), w0, nepochs=500)\n",
    "\n",
    "lambda_ = 150\n",
    "wz_gd_150, vz_gd_150, ez_gd_150 = optimize(gd_l2(y,Xz,lambda_), w0, nepochs=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4b Effect of Prior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Look at the training data log likelihood\n",
    "print('\\nTraining data log likelihood')\n",
    "print('Log-Likelihood for lambda=0  ' + str(l_l2(y, Xz, np.linspace(-5,5,D), 0)))\n",
    "print('Log-Likelihood for lambda=50  ' + str(l_l2(y, Xz, np.linspace(-5,5,D), 50)))\n",
    "print('Log-Likelihood for lambda=100  ' + str(l_l2(y, Xz, np.linspace(-5,5,D), 100)))\n",
    "\n",
    "# Look at the test data log likelihood\n",
    "print('\\nTest data log likelihood')\n",
    "print('Log-Likelihood for lambda=0  ' + str(l_l2(ytest, Xtestz, np.linspace(-5,5,D), 0)))\n",
    "print('Log-Likelihood for lambda=50  ' + str(l_l2(ytest, Xtestz, np.linspace(-5,5,D), 50)))\n",
    "print('Log-Likelihood for lambda=100  ' + str(l_l2(ytest, Xtestz, np.linspace(-5,5,D), 100)))\n",
    "\n",
    "# Print precision, recall and F1-Score for different configurations of lambda_\n",
    "yhat = predict(Xtestz, wz_gd)\n",
    "ypred = classify(Xtestz, wz_gd)\n",
    "print('\\nGradient descent prediction accuracy for lambda=0\\n')\n",
    "print( sklearn.metrics.classification_report(ytest, ypred) )\n",
    "\n",
    "yhat_50 = predict(Xtestz, wz_gd_50)\n",
    "ypred_50 = classify(Xtestz, wz_gd_50)\n",
    "print('\\nGradient descent prediction accuracy for lambda=50\\n')\n",
    "print( sklearn.metrics.classification_report(ytest, ypred_50) )\n",
    "\n",
    "yhat_100 = predict(Xtestz, wz_gd_100)\n",
    "ypred_100 = classify(Xtestz, wz_gd_100)\n",
    "print('\\nGradient descent prediction accuracy for lambda=100\\n')\n",
    "print( sklearn.metrics.classification_report(ytest, ypred_100) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4c Composition of Weight Vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\nGradient descent statistics for lambda=0')\n",
    "print(scipy.stats.describe(wz_gd))\n",
    "\n",
    "print('\\nGradient descent statistics for lambda=50')\n",
    "print(scipy.stats.describe(wz_gd_50))\n",
    "\n",
    "print('\\nGradient descent statistics for lambda=100')\n",
    "print(scipy.stats.describe(wz_gd_100))\n",
    "\n",
    "print('\\nGradient descent statistics for lambda=150')\n",
    "print(scipy.stats.describe(wz_gd_150))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 Exploration (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Try gradient descent on the original data\n",
    "lambda_ = 10\n",
    "w_gd, v_gd, e_gd = optimize(gd_l2(y,X,lambda_), w0, nepochs=500)\n",
    "yhat = predict(Xtest, w_gd)\n",
    "ypred = classify(Xtest, w_gd)\n",
    "print('\\nGradient descent prediction accuracy for lambda=50 without z-scores\\n')\n",
    "print( sklearn.metrics.classification_report(ytest, ypred) )\n",
    "\n",
    "# Leads to numerical instability due to high coefficients and high feature values\n",
    "# The fallback leads for nan of inf is a label of 0.0 which results in a high precision\n",
    "# for 0 values, but this is due to a larger number of non-spam e-mails in the dataset.\n",
    "# This is illustrated by the recall / F1-Score that show a bad prediction for this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run a logistic regression method from some existing library.\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "model = LogisticRegression().fit(Xz, y)\n",
    "ypred = model.predict(Xtestz)\n",
    "print(sklearn.metrics.classification_report(ytest, ypred))\n",
    "\n",
    "# Results in a score of 0.92 which is similar to the score observed with lambda_=50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
